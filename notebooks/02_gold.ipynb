{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03a63d6",
   "metadata": {},
   "source": [
    "### 1. Setup inicial do Spark e Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d354a",
   "metadata": {},
   "source": [
    "#### 1.1 Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52469bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”• Warnings e logs reduzidos.\n",
      "Nenhuma sessÃ£o anterior encontrada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/homebrew/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/hgirardi/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/hgirardi/.ivy2.5.2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-05eca980-dcf8-4060-bcf1-9d37a743f903;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 56ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-05eca980-dcf8-4060-bcf1-9d37a743f903\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n",
      "25/10/03 17:11:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/03 17:11:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark session criada.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings, logging\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Silenciar warnings/logs\n",
    "import warnings, logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for name in [\"py4j\", \"py4j.java_gateway\", \"botocore\", \"boto3\", \"urllib3\"]:\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "print(\"ðŸ”• Warnings e logs reduzidos.\")\n",
    "\n",
    "\n",
    "# carregar variÃ¡veis do .env\n",
    "load_dotenv()\n",
    "\n",
    "# Parar qualquer sessÃ£o Spark existente\n",
    "try:\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "        print(\"Spark session anterior fechada\")\n",
    "    else:\n",
    "        print(\"Nenhuma sessÃ£o anterior encontrada\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao fechar sessÃ£o Spark: {e}\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"IBGE_Gold\")\n",
    "      .config(\"spark.driver.memory\", \"4g\")\n",
    "      .config(\"spark.executor.memory\", \"4g\")\n",
    "      .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "      .config(\"spark.driver.host\", \"localhost\")\n",
    "      .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"âœ… Spark session criada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a3a9d",
   "metadata": {},
   "source": [
    "#### 1.2 Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50194c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de conexÃ£o com o Postgres para usar com o Spark\n",
    "jdbc_url = f\"jdbc:postgresql://{os.getenv('POSTGRES_HOST')}:{os.getenv('POSTGRES_PORT')}/{os.getenv('POSTGRES_DATABASE')}?reWriteBatchedInserts=true\"\n",
    "connection_properties = {\n",
    "    \"user\": os.getenv('POSTGRES_USER'),\n",
    "    \"password\": os.getenv('POSTGRES_PASSWORD'),\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"batchsize\": \"5000\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9bcd8",
   "metadata": {},
   "source": [
    "### 2. Carregar Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdeadcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARTES = 12             # paralelismo na leitura do JDBC\n",
    "FETCHSIZE  = \"50000\"    # tamanho do lote JDBC. Quanto maior, menos idas e vindas ao banco.\n",
    "\n",
    "query = f\"(SELECT *, ntile({PARTES}) OVER (ORDER BY ctid) AS __p FROM silver.covid) t\"\n",
    "\n",
    "df_silver = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", jdbc_url)\n",
    "    .option(\"dbtable\", query)\n",
    "    .option(\"partitionColumn\", \"__p\")\n",
    "    .option(\"lowerBound\", \"1\")\n",
    "    .option(\"upperBound\", str(PARTES))\n",
    "    .option(\"numPartitions\", str(PARTES))\n",
    "    .option(\"fetchsize\", FETCHSIZE)\n",
    "    .options(**connection_properties)\n",
    "    .load()\n",
    "    .drop(\"__p\")\n",
    ")\n",
    "\n",
    "df_silver.createOrReplaceTempView(\"silver_covid\")\n",
    "spark.sql(\"CACHE TABLE silver_covid\")  # acelera as prÃ³ximas queries\n",
    "_ = spark.table(\"silver_covid\").limit(1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72268db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = spark.table(\"silver_covid\").limit(1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe262d20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
